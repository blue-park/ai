{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "실습_3_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQuwLrfneJGq"
      },
      "source": [
        "한국어 자연어처리의 전반적인 FLOW를 이해하고, 간단한 LSTM으로 영화리뷰 감성분석 모델을 훈련합니다\n",
        "\n",
        "필요한 라이브러리를 import해줍니다.\n",
        "\n",
        "* Embedding을 통해 단어를 벡터로 숫자로 바꿔주겠습니다.\n",
        "* LSTM을 활용해 RNN 신경망을 구성합니다.\n",
        "* Dense로 LSTM의 마지막 feature를 연결하여 긍정, 부정으로 분류합니다\n",
        "* pad_sequences는 sequential data의 길이를 padding하거나 clipping하여 일괄 맞추는 전처리를 수행합니다\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5Cnajwnc73B"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM  \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YJfJrzDPG1H"
      },
      "source": [
        "### Step 0. 학습 데이터 준비하기\n",
        "<img src = \"https://github.com/seungyounglim/temporary/blob/master/image_5.PNG?raw=true\">    \n",
        "\n",
        "- 네이버 영화 감성분석 데이터셋 활용\n",
        "- 훈련 데이터 150,000건, 테스트 데이터 50,000건"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHygGkDeiyUZ"
      },
      "source": [
        "\"\"\" 네이버 영화 리뷰 데이터셋 다운로드 \"\"\"\n",
        "!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
        "!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n",
        "\n",
        "\"\"\" 데이터 읽어오기 \"\"\"\n",
        "with open(\"ratings_train.txt\") as f:\n",
        "    raw_train = f.readlines()\n",
        "with open(\"ratings_test.txt\") as f:\n",
        "    raw_test = f.readlines()\n",
        "raw_train = [t.split('\\t') for t in raw_train[1:]]\n",
        "raw_test = [t.split('\\t') for t in raw_test[1:]]\n",
        "\n",
        "FULL_TRAIN = []\n",
        "for line in raw_train:\n",
        "    FULL_TRAIN.append([line[0], line[1], int(line[2].strip())])\n",
        "FULL_TEST = []\n",
        "for line in raw_test:\n",
        "    FULL_TEST.append([line[0], line[1], int(line[2].strip())]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCyE6Ia5uopA"
      },
      "source": [
        "<img src = \"https://github.com/seungyounglim/temporary/blob/master/image_6.PNG?raw=true\">  \n",
        "- 시간 관계상 train 중 50,000건을 학습데이터, 10,000건을 검증 데이터로 사용합니다.\n",
        "- test 중 10,000건만 샘플링하여 최종 성능 테스트에 사용하겠습니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_xG06TPI9vl"
      },
      "source": [
        "import random\n",
        "random.seed(1)\n",
        "random.shuffle(FULL_TRAIN)\n",
        "random.shuffle(FULL_TEST)\n",
        "train = FULL_TRAIN[:50000]\n",
        "val = FULL_TRAIN[50000:60000]\n",
        "test = FULL_TEST[:10000]\n",
        "print(\"train     : {}개 (긍정 {}, 부정 {})\".format(len(train), sum([t[2] for t in train]), len(train)-sum([t[2] for t in train])), train[0])\n",
        "print(\"validation: {}개 (긍정 {}, 부정 {})\".format(len(val), sum([t[2] for t in val]), len(val)-sum([t[2] for t in val])), val[0])\n",
        "print(\"test      : {}개 (긍정 {}, 부정 {})\".format(len(test), sum([t[2] for t in test]), len(test)-sum([t[2] for t in test])), test[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1IWVSphmZ42"
      },
      "source": [
        "라벨을 보니 0이 부정 리뷰, 1이 긍정 리뷰입니다. \n",
        "\n",
        "한국말은 끝까지 읽어봐야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RfgQ96FEjKz"
      },
      "source": [
        "### Step 1. Tokenizing(Parsing)\n",
        "- 문장을 음절(character)단위로 쪼갭니다\n",
        "- 예시 문장 두 개를 넣어서 어떻게 쪼개지는지 알 수 있습니다\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXHdDffgaggl"
      },
      "source": [
        "def tokenize(sentence): \n",
        "  return [char for char in sentence]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_qXa9a0Ehzv"
      },
      "source": [
        "tokenize(\"문장을 한 글자씩 쪼개줍니다.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jykZkziwnd2"
      },
      "source": [
        "Train/ Test의 문장을 음절단위로 나눕니다\n",
        "\n",
        "정답 라벨은 부정(0), 긍정(1)로 이루어져있습니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9HFloKROOnT"
      },
      "source": [
        "train_sentences = []\n",
        "val_sentences = []\n",
        "test_sentences = []\n",
        " \n",
        "train_label_ids = []\n",
        "val_label_ids = []\n",
        "test_label_ids = []\n",
        " \n",
        "for i, line in enumerate(train):\n",
        "    words = tokenize(line[1])\n",
        "    train_sentences.append(words) \n",
        "    train_label_ids.append(line[2])   \n",
        "\n",
        "for line in val:\n",
        "    words = tokenize(line[1])\n",
        "    val_sentences.append(words) \n",
        "    val_label_ids.append(line[2])  \n",
        " \n",
        "for line in test:\n",
        "    words = tokenize(line[1])\n",
        "    test_sentences.append(words) \n",
        "    test_label_ids.append(line[2])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58bDXwmsH2x6"
      },
      "source": [
        "##Step 2. 모델 인풋 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLSdN7ROFdwN"
      },
      "source": [
        "#### 2-1) 음절 사전 만들기\n",
        "각 음절을 모델이 처리할 수 있는 정수 인덱스로 변환해야 합니다.\n",
        "- 훈련 데이터 문장에 있는 음절을 정수로 매핑하는 사전을 만들고,\n",
        "- 배치 연산을 위해 필요한 Padding([PAD])과 Out of vocabulary([OOV]) 토큰을 항상 맨 앞에 추가해줍니다\n",
        "\n",
        "(일반적으로는 더 많은 코퍼스에 대해 구축된 사전을 사용하지만, 편의상 훈련셋만으로 진행합니다)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAfNMCpgwdww"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "vocab_dict = {}\n",
        "vocab_dict[\"[PAD]\"] = 0\n",
        "vocab_dict[\"[OOV]\"] = 1\n",
        "i = 2\n",
        "for sentence in train_sentences:\n",
        "    for word in sentence:\n",
        "        if word not in vocab_dict.keys(): \n",
        "            vocab_dict[word] = i\n",
        "            i += 1\n",
        "print(\"Vocab Dictionary Size:\", len(vocab_dict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu_DabQjQYFK"
      },
      "source": [
        "#### 2-2) vocab_dict를 이용해 자연어를 정수 인덱스로 바꾸기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCyqBSGYFjXx"
      },
      "source": [
        "#### 2-2) vocab_dict를 이용해 자연어를 정수 인덱스로 바꾸기\n",
        "- 위에서 만든 vocab_dict를 이용해 잘라놓은 문장을 모델에 태울 수 있는 정수 인덱스로 바꾸어줍니다\n",
        " \n",
        "    - 이 때, 사전에서 매핑되는 음절은 해당 인덱스로 바꾸고 사전에 없는 음절은 [OOV] 인덱스로 처리합니다.\n",
        "\n",
        "- 기본적으로 LSTM은 가변적인 문장 길이를 인풋으로 받을 수 있지만, 배치 처리를 위해 <font color=\"blue\">max_seq_len</font>을 정해두고 길이를 통일합니다.\n",
        "    - max_seq_len 보다 짧은 문장에는 max_seq_len이 될 때까지 [PAD]에 해당하는 인덱스를 붙여줍니다\n",
        "    - max_seq_len 보다 긴 문장은 max_seq_len 개의 토큰만 남기고 자릅니다\n",
        "\n",
        "기본적으로 [PAD]는 시퀀스의 앞에 붙이는 것이 관례입니다. 앞부분이 상대적으로 RNN 모델이 잊어버릴 가능성이 크기 때문에, 쓸모없는 부분은 앞으로 몰아넣습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHI2uFCUojkw"
      },
      "source": [
        "def make_input_ids(tokenized_sentences, max_seq_len = 50):\n",
        "  \n",
        "  num_oov = 0 # OOV 발생 개수를 셈\n",
        "  result_input_ids = [] # result_input_ids : 정수 인덱스로 변환한 문장들의 리스트\n",
        "\n",
        "  for sentence in tokenized_sentences :\n",
        "      \"\"\" vocab_dict를 사용해 정수로 변환 \"\"\" \n",
        "      input_ids = []\n",
        "      for word in sentence:\n",
        "          if word not in vocab_dict:   ## 사전에 없는 음절은 OOV 처리\n",
        "              input_ids.append(vocab_dict['[OOV]']) \n",
        "              num_oov += 1\n",
        "          else:                       ## 사전에 있는 음절은?\n",
        "              input_ids.append(vocab_dict[word]) ##  vocab_dict 사전에서 토큰 찾아서 붙이기\n",
        "      \n",
        "      result_input_ids.append(input_ids)\n",
        "      \n",
        "  \"\"\" max_seq_len을 넘는 문장은 절단, 모자르는 것은 PADDING \"\"\"\n",
        "  result_input_ids = pad_sequences(result_input_ids, maxlen=max_seq_len, value=vocab_dict[\"[PAD]\"]) ##  padding 하기\n",
        "\n",
        "  return result_input_ids, num_oov\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Irm8oGg3ICWV"
      },
      "source": [
        "# train_sentences 처리\n",
        "train_input_ids, num_oov = make_input_ids(train_sentences)\n",
        "\n",
        "print(\"---- TRAIN ----\")\n",
        "print(\"... # OOVs     :\", num_oov)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yFz1GGuxhGY"
      },
      "source": [
        "# val_sentences 처리\n",
        "val_input_ids, num_oov = make_input_ids(val_sentences)\n",
        "\n",
        "print(\"---- VALIDATION ----\")\n",
        "print(\"... # OOVs     :\", num_oov)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYs0YXRjIDg3"
      },
      "source": [
        "# test_sentences 처리\n",
        "test_input_ids, num_oov = make_input_ids(test_sentences)\n",
        "\n",
        "print(\"---- TEST ----\")\n",
        "print(\"... # OOVs     :\", num_oov)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7l98rF7c-Er"
      },
      "source": [
        "#### 2-3) 라벨 리스트를 np.array로 변환해줍니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuFQFHbj3AQd"
      },
      "source": [
        "train_label_ids = np.array(train_label_ids)\n",
        "val_label_ids = np.array(val_label_ids)\n",
        "test_label_ids = np.array(test_label_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhcHsul1IHxj"
      },
      "source": [
        "## Step3. 모델 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39iPCdwGTjzk"
      },
      "source": [
        "\n",
        "LSTM을 사용해 문장을 인코딩하고, Dense layer을 쌓아 최종 output을 생성합시다\n",
        "\n",
        "# 실습 MISSION : RNN 모델 하이퍼파라미터 변경하기\n",
        "\n",
        "여러 설정들을 마음대로 변경해봅시다.\n",
        "\n",
        "- 임베딩 레이어 : 음절을 몇 차원의 벡터로 변경할지 조정합니다.\n",
        "- LSTM : LSTM의 hidden size를 조정합니다.\n",
        "- Dense : 추가, 삭제, 노드 수를 조정합니다.\n",
        "- 기타 regularization 효과를 원하면 import하고 추가합니다\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMgZkCRJSaF2"
      },
      "source": [
        "vocab_size = len(vocab_dict)        # 단어사전 개수\n",
        "embedding_dim = 64     # 임베딩 size\n",
        "lstm_hidden_dim = 50   # LSTM hidden_size \n",
        "dense_dim = 64         # Dense layer size\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim),\n",
        "    LSTM(lstm_hidden_dim),\n",
        "    Dense(dense_dim, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GXrD_dTdTa_"
      },
      "source": [
        "# Step 4. 모델 훈련하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPp_wZ2ddaMG"
      },
      "source": [
        "loss, optimizer를 지정하고 학습합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3nPJiGW1q92"
      },
      "source": [
        "EPOCHS = 10\n",
        "BATCHS = 128\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(train_input_ids, train_label_ids, epochs=EPOCHS, batch_size=BATCHS, validation_data=(val_input_ids, val_label_ids), verbose=2) \n",
        "\n",
        "test_result = model.evaluate(test_input_ids, test_label_ids, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42MnPFs9O3Fr"
      },
      "source": [
        "학습 결과를 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtaGxpJa4SmM"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "  \n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13AeKZe8JdIn"
      },
      "source": [
        "원하는 문장으로 결과를 추론해봅니다.\n",
        "\n",
        "스코어가 0에 가까울수록 부정, 1에 가까울수록 긍정입니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxopYLQtIkw1"
      },
      "source": [
        "\"\"\" 학습된 모델로  예측해보기 \"\"\"\n",
        "\n",
        "def inference(mymodel, sentence):\n",
        "  # 1. tokenizer로 문장 파싱\n",
        "  words = tokenize(sentence)\n",
        "  input_id = []\n",
        "\n",
        "  # 2. vocab_dict를 이용해 인덱스로 변환\n",
        "  for word in words:\n",
        "    if word in vocab_dict: input_id.append(vocab_dict[word])\n",
        "    else: input_id.append(vocab_dict[\"[OOV]\"])\n",
        "  \n",
        "  # 단일 문장 추론이기 때문에 패딩할 필요가 없음 \n",
        "  score = mymodel.predict(np.array([input_id])) \n",
        "\n",
        "  print(\"** INPUT:\", sentence, end=\"\")\n",
        "  print(\" ->  {:.2f}\".format(score[0][0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mi_fIiVC4k8g"
      },
      "source": [
        "# 원하는 문장에 대해 추론해 보세요 \n",
        "inference(model, \"안보면 후회ㅠㅠ...\")\n",
        "inference(model, \"이런 망작을 나 혼자만 보기엔 아깝지\")\n",
        "inference(model, \"이런 꿀잼을 나 혼자만 보기엔 아깝지\")\n",
        "inference(model, \"꿀잠 잤습니다\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Y_E831p2KJ8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}